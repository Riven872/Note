## Java

### 1、synchronized 和 reentrantLock区别？

1. 都是作用于线程间的同步控制。而且都是可重入锁。
2. synchronized 是 Java 内置的特性，是一个关键字。Lock 是 JUC 包中的工具类，通过代码实现。
3. synchronized 是自动获取 / 释放锁。Lock 需要手动的 lock 和 unlock。
4. synchronized 只有非公平锁。Lock 可以实现公平锁也可以实现非公平锁。
5. Lock 有响应中断、超时等待以及其他的特性。



### 2、如何理解 AQS？

1. 是抽象的队列同步器，是抽象类，因此是很多同步器的基础框架。如 ReentrantLock、CountDownLatch 和 Semaphore 都是基于 AQS 实现。
2. 在 AQS 内部，维护了一个队列（FIFO）和一个 volatile 的 int 类型的 state 变量。
    1. 在 state 为 1 时表示当前对象的锁已经被某个线程占有了。
    2. 队列中的每个结点表示一个线程，保存着线程的引用、当前结点的状态（是否有获取到对象锁）。
    3. 因此一条队列就是所有请求该临界资源的线程，一个对象锁对应一条队列。
    4. AQS 的目的在于多个线程竞争临界资源时，将暂时没有获取到锁的线程放到队列中进行阻塞等待，而被唤醒时进行锁的分配。
3. AQS 提供了两种锁的机制，共享锁（读锁）和排它锁（写锁）
    1. 如 ReentrantLock 的可重入就是 AQS 的排它锁功能。
    2. 如 CountDownLatch 和 Semaphore 使用到了 AQS 的共享锁功能。



### 3、AQS 是如何维护 CLH 队列的。

1. 公平锁：当前锁已经被其他线程持有时，新的线程会创建一个结点然后加入到 CLH 队列的尾部自旋等待，也就是按照请求锁的先后顺序加入队列。当锁释放时，会通知队列的下一个结点，这个节点在获取锁后，再通知下一个节点，以此类推。这样保证了队列中的线程按照 FIFO 的顺序逐个获取锁，实现公平性。
2. 非公平锁：与公平锁的区别时，如果一个线程还没有加入到 CLH 队列，第一次获取锁时，会与当前持有锁的线程同时竞争锁的拥有权。竞争的结果取决于具体实现的锁策略，可能是先到先得，也可能是其他策略。这意味着非公平锁不会强制等待线程按照它们进入等待队列的顺序获取锁。只有在竞争失败时，才会加入到 CLH 队尾。不一定遵循先到先得的原则。



### 4、synchronized 的锁升级过程是怎样的？

1. 无锁：当一个线程第一次访问临界资源时，JVM 在这个对象头中设置该线程的 ID，并将对象头的状态位设置为 “偏向”。
2. 偏向锁：只有一个线程访问临界资源时，该线程不需要同步操作。当有其他线程访问时，会检查对象头 “偏向锁” 状态和线程的 ID，如果相同则直接获取锁，如果不同则会升级为轻量锁状态。
3. 轻量级锁：在此状态时，JVM 将对象头中的标识设置为 “轻量级锁” 状态。当一个线程成功获取到对象锁时，会将对象头中的线程栈中存放自己的线程指针。此时如果有另一个线程想要获得对象锁，且对象处于 “轻量级锁” 状态时，会尝试使用 CAS 将对象头中的线程栈替换成自己的指针，如果成功则获得锁。如果失败则表示该锁已被其他线程获取，该锁状态会升级为重量级锁状态。
4. 重量级锁：在此状态时，JVM 在对象头中记录指向 CLH 队列的指针。当锁被释放时，根据规则从 CLH 队列中选择一个线程唤醒并获得锁，并将该线程的状态从 “等待” 置为 “就绪”。
5. 锁升级是线程竞争逐渐变多时，对象标识不断更改并使获取锁的难度不断增加的过程。
6. 其中，AQS 管理的是重量级锁，因为涉及到线程的阻塞和唤醒操作。



### 5、讲一下 JMM 内存模型。

1. JMM 为了屏蔽各种硬件和操作系统的访问差异，保证 Java 程序在各种平台下对内存的访问都能保持效果一致的规范和机制。
2. JMM 规定了所有的变量都存储在主内存中，每条线程有自己的工作内存，线程的工作内存中保存了该线程中使用到的变量的主内存副本的拷贝。线程对变量的操作都必须在自己的工作内存中进行，而不能直接读写主内存。不同线程之间无法直接访问对方的工作内存，需要主内存当桥梁。
3. 如果没有 Volatile 的修饰，变量刷回主内存的情况
    1. 线程退出临界资源时，会将工作内存中所有修改过的变量值刷新回主内存。
    2. 线程进入临界资源时，会先清空工作内存中的共享变量值，从主内存中读取最新的变量值。



### 6、实现多线程的方式有哪些？

1. 继承 Thread 类创建线程。
2. 实现 Runnable 接口创建线程。
3. 通过 Callable 和 FutureTask 创建线程。
4. 通过线程池创建线程。
5. Callable 可以使用本身的 `call()` 方法来获取返回值，但是并不灵活。如果直接调用 `call()`，则相当于一次同步调用，并且阻塞在当前位置
6. FutureTask 是 Future 接口的一个实现，通常使用 Future 来管理异步任务，比如通过线程池执行异步任务时，可以使用 Future 接收该任务：`Future<Integer> future = executorService.submit(callable);` ，并在需要返回值时进行 get 即可，也可以查询该任务的状态是否已经完成。
7. 因此需要与 FutureTask 需要配合使用，`Future<String> future = new FutureTask<>(callable);`



### 7、在线程池中，怎么把执行时间不一致的线程按照规定的顺序执行？（多线程编排问题）

1. 首先需要使用 Future 接口的实现类 CompletableFuture 去实现该功能。
2. CompletableFuture 可以通过 `supplyAsync()` 创建异步任务，并使用 `thenApply()` 在得到上一个异步任务的结果之后执行下一步的操作，也可以通过 `get()` 直接获取该异步任务的结果，或者使用 `handle()` 处理发生的异常。更复杂的还有 `thenCombine()` 等组合多个异步任务的结果。
3. 可以指定自定义的线程池，如果不指定，则会使用默认的 ForkJoinPool 线程池。



### 8、ConcurrentHashMap 是如何保证线程安全的？

1. 使用分段锁 + CAS + synchronized 的机制来保证线程安全。
2. 当 ConcurrentHashMap 在添加或删除元素时，首先使用 CAS 来尝试修改元素，如果 CAS 操作失败，就会使用 synchronized 锁住当前的哈希槽，然后再次尝试修改或删除元素。
3. 使用 synchronized 时使用了分段机制，只是锁住了当前的哈希槽，并非整个集合，降低了锁的颗粒度。
4. 而且 ConcurrentHashMap 不允许有 value 为 null 的情况。如果使用 `map.get(key)` 返回了 null 值，是不明确本来就存了一个 null 值进去还是因为没有找到而返回了 null，存在二义性。而 HashMap 在单线程下是不存在这个问题的，所以 HashMap 允许 value 为 null 的情况。



### 9、ThreadLocal 了解吗？发生内存泄漏的原因？

1. `ThreadLocal` 实例通过 `ThreadLocalMap` 对象与当前线程建立关联的底层结构：`<Thread, Entry<ThreadLocal, Object>>`。即每一个线程有独立的 Entry，这个 Entry 中以 ThreadLocal 实例对象为 key，该 ThreadLocal 实例对象中存放的值为 value。（其中 `Entry<ThreadLocal, Object>` 称为 `ThreadLocalMap`，因此实际上数据存放在 ThreadLocalMap 中）
2. 其中 ThreadLocal 对象有两处引用：
    1. 在栈内存中，使用 ThreadLocal 对象方法时，对 ThreadLocal 的引用。（强引用）
    2. 在堆内存中，ThreadLocalMap 中的 key 对 ThreadLocal 的引用。（弱引用）
3. 发生 OOM 的两个原因：
    1. 当栈内存中的 ThreadLocal 引用不再使用，也就是方法结束后这个对象引用就不存在了，但因为还存在着一条引用链导致无法 GC，也就可能导致 OOM。因此设计时采用了弱引用，即这个对象只具有弱引用时，就会被 GC，防止 OOM。
    2. 但要注意另一个 OOM 的点是：key 虽然是 ThreadLocal 弱引用，可以被 GC，但值是强引用的，会存在 ThreadLocal 被 GC 了，但是 value 依然存在，然后导致 OOM。因此需要每次使用完 ThreadLocal 后，手动调用一下 remove，就会在下一次 GC 时，清理掉失效过期的 Entry。（当然，每次调用 ThreadLocal 的 get、set 时也会清理，但最好还是手动 remove 一下）
4. 在配合线程池使用时，更需要注意清理 ThreadLocal 内的数据。因为线程池的线程是复用的，而且在 ThreadLocal 中，一个线程对应自己的 Entry。如果上一个线程使用完之后没有清理 ThreadLocal，那么再次使用这个线程时，就会出现数据错乱的情况，仍然具有上一个线程所具有的数据。因此需要及时清理，也可以预防 OOM 的问题。



## MySQL

### 1、B+ 树，和 B 树有什么区别？

1. 二者都是 AVL 树的一种，又叫平衡多路查找树。
2. B 树的所有节点既存放键 key，也存放数据 Data；而 B+ 树只有叶子节点存放 key 和 Data，非叶子节点只存放 Key。因此 B+ 树的任何查找必须从根结点到叶子结点，所以 B+ 树的检索顺序是稳定的。
3. B 树的叶子节点是相互独立的；而 B+ 树的叶子节点之间有双向链表连接。
4. B+ 树效率更高
    1. 因为 B+ 树的所有非叶子节点只保存 key，因此大小同样的磁盘可以保存更多的节点元素，层数更少，IO 操作也更少
    2. B+ 树检索顺序稳定，性能也稳定



### 2、说一说 MySQL 一条 SQL 语句的执行过程？

1. 连接：使用连接器，与 MySQL 服务器建立连接，并查询是否有权限。
2. 解析：由解析器进行语法和语义的分析，并生成解析树。如查询的是什么表、哪个字段、条件是什么。
3. 优化：由优化器生成执行计划，并根据索引查看是否可以优化。
4. 执行：执行器执行 SQL 语句，根据指定的存储引擎执行并得到查询结果。



### 3、当前读和快照读有什么区别？

1. 快照读就是在普通的 SELECT 语句在不加锁的情况下查询的数据。
2. 当前读就是加锁的 SELECT 或者对数据进行 DDL 操作时，每次读取都是最新数据。
3. 只有在 RC 和 RR 隔离级别时才会使用快照读。
4. 在 RC 中，每次读取都会重新生成一个新的快照（和一个新的 Read View 接下题），即总是读取行的最新版本。因此可能会出现不可重复读的情况。
5. 在 RR 中，快照会在事务中第一次 SELECT 语句执行时生成，只有在本事务中对数据进行更改才会更新快照（并更新 Read View 接下题）。因此解决了不可重复度的情况，也只解决了快照读下的幻读问题。
6. Read View 决定了哪条快照读，因此生成时机是相同的，顺序是先后的关系。



### 4、讲一下 MVCC 机制。

1. 目的：不使用悲观锁的情况下，控制读写并发并保持相对较高的性能。
2. 快照读：是 MVCC 实现的基础。（快照存放在哪里？见3）
3. undo log 日志：是实现 MVCC 的重要手段。因为事务没有提交之前，会将更新前的数据先放到日志中，而更新前的数据就是快照。（在日志中，一条记录可能有多个快照，那么在需要快照时，要读取哪个快照呢？见4）
4. 行记录的隐藏字段：最新一次修改的事务的 id、回滚指针指向日志中上一个版本的快照。这样就形成了一个快照链表，新 -> 旧 -> 更旧，方便回滚。（知道了快照之间的联系，但仍不知道具体要哪个快照，见5）
5. Read View：
    1. 生成时机：使用快照读时生成 Read View 表并决定使用哪条快照。
    2. 保存了正处于活跃未提交的事务 ID。
    3. 用来解决可见性问题，会告诉我们本次事务哪些事务可见，哪些不可见。
    4. 每开启一个事务，都会从数据库中得到一个事务 ID，事务的 ID 是自增的，因此可以通过 ID 大小来判断事务开启的时间顺序。
    5. 事务 ID 大的可以看到事务 ID 小的事务的变更结果，因此拿到一条可见的记录后就作为快照读返回。
    6. 如果没有可见的事务，就去 undo log 中根据快照链表依次取快照，并进行事务 ID 的对比判断，直到找到可见的事务并返回快照读，如果找不到就返回空。
6. 总结：undo log 用来保存历史快照，Read View 用来判断哪个快照是可见的。



### 5、QPS、TPS、RT 代表什么？

1. QPS：每秒接收了多少请求。（流量进来了多少）
2. TPS：接收请求之后实际每秒能处理多少。（服务端响应了多少）
3. RT：响应时间，response time。



## Redis

### 1、Hash 结构的底层是什么数据结构？

1. 数组 + 链表，基本与 Java 的 HashMap 结构差不多，但是没有树化。



### 2、缓存更新策略。

1. 旁路缓存模式（Cache Aside Pattern）
    1. 规定写操作的步骤为：更新数据库中的值、直接删除缓存（写操作不重建）。
    2. 规定读操作的步骤为：读取缓存的数据，如果存在则直接返回、没有命中则查询数据库、重建缓存。
    3. 会出现数据不一致性，但几率极低，例子：读请求读取缓存，值（15）不存在，根据规定步骤然后查询数据库取值（15），同时写请求并发，更新了该值（15 -> 20），根据规定并删除了缓存（假设此时没有重建完成），最后读请求完成缓存的重建（15）。两个请求完成后出现了数据不一致（数据库 20，缓存 15）。
        1. 查询的值不在缓存中（即缓存失效没有命中该值）。
        2. 读请求、写请求并发。
        3. 写请求更新数据库的时间要比读请求查询数据库 + 重建缓存的时间还要短。
2. 缓存延迟双删策略（对旁路缓存的优化）
    1. 规定写操作的步骤为：更新数据库中的值、直接删除缓存、休眠一段时间后再删除缓存（延迟删除给更新数据库充足的时间）。
    2. 读操作步骤不变。
    3. 也可以结合消息队列，在更新完数据库之后发送一条延时信息给队列，通知消费者删除缓存，从而达到延迟的效果（但增加了系统的复杂度）。



### 3、讲一下 RDB 持久化机制。

1. 规定在 n 秒之后，如果至少有 m 个 key 发生了变化时，Redis 就会自动触发 bgsave 命令创建当前时间点上的数据库的副本，并将副本持久化。
2. bgsave 会 fork 一个主进程进行异步生成快照，而 save 会阻塞主进程。
3. fork 采用的是写时复制技术（copy on write）
    1. 主进程执行读操作时，可以访问共享内存
    2. 主进程执行写操作时，会先拷贝出一份数据，执行写操作，此时 fork 进程可以读不受影响，主进程写完之后合并到内存。
4. 优点：快照文件小、恢复速度快，适合做备份和灾难恢复。
5. 缺点：定期更新可能会丢失数据。



### 4、讲一下 AOF 持久化机制。

1. 将 Redis 所有写操作追加到 AOF 文件的末尾，从而记录了 Redis 运行期间所有修改操作的记录。
2. AOF 有三种刷盘策略
    1. 同步回写：每个写命令执行完时立即刷盘。
    2. 每秒回写：每个写命令执行完，只是先把日志写到 AOF 的内存缓冲区，每隔一秒才把缓冲区中的内容刷盘。
    3. 操作系统控制：每个写命令执行完，只是先把日志写到 AOF 的内存缓冲区，由操作系统决定何时刷盘。
3. 优点：实现了更高的数据可靠性、支持更细粒度的数据恢复，适合做数据存档和数据备份。
4. 缺点：文件大占用空间多，每次写操作都需要写磁盘导致负载高。



### 5、全量同步和增量同步对应条件

1. 全量同步：初次建立主从关系时（replicaof 命令），从节点需要从主节点上复制所有的数据
    1. slave 节点请求数据同步。
    2. master 节点判断是否为第一次同步，如果是则执行全量同步，master 节点先发送主节点的 replid 和偏移量。
        1. 每个节点都有唯一的 replid 标识，未同步之前是不同的，同步后 slave 会集成 master 的 replid，从而判断是否是第一次同步。
    3. slave 节点接收 master replid 和偏移量并保存。
    4. master 节点执行 bgsave 生成 RDB，并发送给 slave 节点，在此期间如果有新的命令则写入到 repl_baklog 中。
        1. repl_baklog 的记录越多主节点的偏移量就越大，slave 完成同步时也会记录此时的偏移量，如果 slave 的偏移量小于 master 的偏移量，说明 slave 数据落后于 master，则需要更新。
    5. slave 节点清空本地数据并加载新的 RDB 文件。
    6. master 节点发送 repl_baklog 中的命令，slave 节点执行接收到的命令。
2. 增量同步：当主节点和从节点之间的网络断开，再次重新连接时，从节点只会请求主节点上部分缺失的数据，以减少同步数据量，提高同步效率和稳定性
    1. slave 节点向 master 节点同步 replid 和偏移量。
    2. master 节点判断请求的 replid 是否一致，是否不是第一次同步。
    3. 判断成功后，master 取 repl_baklog 中 slave 偏移量之后的命令并发送给 slave 节点。
    4. slave 节点执行命令进行增量同步。
3. 注意：repl_baklog 是环形数组大小有上限，写满之后会覆盖最早的数据。如果 slave 断开时间太久，偏移量被未同步的数据覆盖（或者说是 master 节点的偏移量套圈了 slave 的偏移量），就无法再做 repl_baklog 增量同步（因为未同步的数据已经被覆盖丢失了），此时只能做全量同步。



### 6、Redis 分布式锁在多线程下会遇到哪些问题？

1. 锁无法续期：线程 A 执行时间超过了锁的超时释放时间，会出现线程 A 在执行但是锁已经自己释放的问题。
2. 锁误删问题：线程 A 执行时间超过了锁的超时释放时间，锁超时释放，此时另一个线程 B 会正常获取锁，线程 A 执行完毕后，会执行释放锁的命令，此时会将线程 B 的锁释放掉，出现锁误删的问题。
    1. 解决方法：使用 UUID 标识唯一线程，只有同一线程删除锁时才会释放。
3. 不可重入：线程 A 获取到锁之后，如果遇到再获取锁的场景，无法获取该锁，造成业务无法执行。
    1. 解决方法：使用 Hash 结构处理，K-V 代表线程和重入次数。
4. 推荐使用 Redisson 去实现分布式锁。
    1. 锁的结构：key 是自定义的锁的名称，field 是线程 id，value 是该线程的加锁次数。（因为不是 String，没有 SETNX 的特性，因此需要在 Lua 脚本中执行获取、释放锁的流程）
    2. 获取锁：使用 Exist 判断该锁的值是否为 null，如果为 null 则正常获取锁，并将 value 值设置为当前线程的 ID 或者增加重入次数，否则 value 值就是已经获取锁的线程 ID。（实现了锁的互斥、可重入）
    3. 释放锁：校验 value 值是否为当前线程的 ID，如果是则正常释放。（实现了锁的正确释放）
    4. 自动续约：使用看门狗机制，也就是一个定时程序，只要业务持有 key 并且还在执行，就会不断的重置 TTL，防止锁超时释放但业务没有执行完。（如果业务挂了，看门狗线程也就没有了，因此不会出现业务挂了仍无限续约锁的情况）